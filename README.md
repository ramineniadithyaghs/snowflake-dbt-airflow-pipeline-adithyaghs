# Snowflake ELT Pipeline with dbt and Airflow

## Architecture Diagram

![Architecture Diagram](https://github.com/ramineniadithyaghs/Snowflake-ELT-Pipeline-with-dbt-and-Airflow/blob/main/Images/Architecture_Adithya.png)

## Airflow DAG

![Airflow DAG](https://github.com/ramineniadithyaghs/Snowflake-ELT-Pipeline-with-dbt-and-Airflow/blob/main/Images/dbt_dag2.png)

## Data Flow Diagram

![DataFlowDiag](https://github.com/ramineniadithyaghs/Snowflake-ELT-Pipeline-with-dbt-and-Airflow/blob/main/Images/DataFlowDiagram.png)

A production-grade ELT (Extract, Load, Transform) data pipeline that processes millions of records using modern data stack best practices. This project demonstrates enterprise-scale data engineering with Snowflake, dbt, and Apache Airflow.
ğŸ“Š Architecture Overview
Show Image
Pipeline Flow

Extract: Raw data from TPCH orders table (Snowflake sample database)
Load: Ingest into Snowflake data warehouse
Transform: dbt models create staging, intermediate, and mart layers
Orchestrate: Airflow DAGs automate the entire workflow
Test: Automated data quality checks ensure integrity


ğŸ¯ Key Features
âœ… Modular dbt Project

Source definitions and documentation
Staging models for raw data cleaning
Intermediate models for business logic
Fact tables and data marts for analytics
Custom macros (e.g., discount price calculations)

âœ… Comprehensive Testing

Generic tests (uniqueness, not_null, relationships)
Singular tests for business logic validation
Schema validation and data quality checks

âœ… Production-Ready Orchestration

Airflow DAGs with retry logic
Task dependencies and error handling
Scheduled runs and monitoring

âœ… Best Practices

Medallion architecture (Bronze â†’ Silver â†’ Gold)
DRY principles with dbt macros
Version control and CI/CD ready
Clear documentation and lineage


ğŸ› ï¸ Tech Stack
ComponentTechnologyPurposeData WarehouseSnowflakeStorage and computeTransformationdbt (Data Build Tool)SQL-based transformationsOrchestrationApache AirflowWorkflow automationLanguagePython 3.9+Scripting and automationVersion ControlGit/GitHubCode management

ğŸ“ Project Structure
snowflake-dbt-airflow-pipeline/
â”‚
â”œâ”€â”€ data_pipeline/           # Core dbt project
â”‚   â”œâ”€â”€ models/             # dbt models (staging, marts)
â”‚   â”œâ”€â”€ macros/             # Reusable SQL functions
â”‚   â”œâ”€â”€ tests/              # Data quality tests
â”‚   â””â”€â”€ dbt_project.yml     # dbt configuration
â”‚
â”œâ”€â”€ dbt-dag/                # Airflow DAGs
â”‚   â””â”€â”€ dbt_pipeline.py     # Orchestration logic
â”‚
â”œâ”€â”€ Images/                 # Architecture diagrams
â”‚   â”œâ”€â”€ Architecture_Adithya.png
â”‚   â”œâ”€â”€ dbt_dag2.png
â”‚   â””â”€â”€ DataFlowDiagram.png
â”‚
â””â”€â”€ README.md              # This file

ğŸš€ Getting Started
Prerequisites

Snowflake Account (free trial available)
Python 3.9+
dbt-snowflake (pip install dbt-snowflake)
Apache Airflow 2.0+
Git

Installation

Clone the repository

bashgit clone https://github.com/ramineniadithyaghs/snowflake-dbt-airflow-pipeline-adithyaghs.git
cd snowflake-dbt-airflow-pipeline-adithyaghs

Set up dbt profile

bash# Create ~/.dbt/profiles.yml
nano ~/.dbt/profiles.yml
Add your Snowflake credentials:
yamldata_pipeline:
  target: dev
  outputs:
    dev:
      type: snowflake
      account: YOUR_ACCOUNT
      user: YOUR_USERNAME
      password: YOUR_PASSWORD
      role: YOUR_ROLE
      database: YOUR_DATABASE
      warehouse: YOUR_WAREHOUSE
      schema: YOUR_SCHEMA
      threads: 4

Install dependencies

bashpip install dbt-snowflake apache-airflow

Test dbt connection

bashcd data_pipeline
dbt debug

Run dbt models

bashdbt run
dbt test

ğŸ“Š Visualizations
Airflow DAG
Show Image
Orchestrated workflow showing task dependencies and execution flow
Data Flow
Show Image
End-to-end data transformation journey from source to analytics

ğŸ§ª Data Quality & Testing
This project includes comprehensive testing:
Generic Tests:

Uniqueness checks on primary keys
Not-null validation on required fields
Referential integrity between tables
Accepted value ranges

Singular Tests:

Custom business logic validation
Cross-model consistency checks
Revenue calculation accuracy

Run tests:
bashdbt test

ğŸ“ˆ Key Transformations

Source Layer (sources.yml)

Defines raw TPCH orders table
Documents data lineage


Staging Layer (stg_orders.sql)

Cleans raw data
Standardizes column names
Applies data types


Intermediate Layer (int_orders_enhanced.sql)

Business logic calculations
Uses custom macros (discount pricing)
Aggregations and joins


Mart Layer (fct_orders.sql)

Analytics-ready fact table
Optimized for BI tool consumption
Pre-aggregated metrics

ğŸ”„ Airflow Orchestration
The pipeline is automated with Airflow:
DAG Features:

âœ… Scheduled daily runs
âœ… Automatic retries on failure
âœ… Email alerts for errors
âœ… Task dependencies ensure correct execution order
âœ… SLA monitoring

Start Airflow:
bashairflow db init
airflow webserver --port 8080
airflow scheduler

ğŸ’¡ Use Cases
This pipeline architecture can be adapted for:

E-commerce Analytics - Order processing and customer insights
Financial Reporting - Transaction data transformation
Supply Chain - Inventory and logistics tracking
Marketing Analytics - Campaign performance measurement

ğŸ“ What I Learned
Building this project taught me:
âœ… Modern data stack best practices (Snowflake + dbt + Airflow)
âœ… Incremental model design for performance
âœ… Data testing and validation strategies
âœ… Workflow orchestration and error handling
âœ… Documentation and maintainability principles

ğŸ”® Future Enhancements
Planned improvements
 Add dbt docs generation (dbt docs generate)
 Implement incremental models for large datasets
 Add data quality monitoring with Great Expectations
 CI/CD pipeline with GitHub Actions
 Snowflake cost optimization (clustering, materialization)
 Integration with BI tools (Tableau/Power BI)
 
ğŸ“ License
This project is licensed under the MIT License - see the LICENSE file for details
ğŸ‘¤ Author
Adithya Ramineni
LinkedIn: linkedin.com/in/adithyaramineni
Email: adithyachowdaryr@gmail.com
Portfolio: More projects on GitHub
ğŸ™ Acknowledgments
Snowflake for the TPCH sample database
dbt Labs for excellent documentation
Apache Airflow community for orchestration tools
ğŸ“« Contact
Have questions or want to collaborate? Feel free to reach out!
â­ If you found this project helpful, please star this repository! â­

Built with â¤ï¸ by a data engineer passionate about building reliable, scalable data infrastructure.
